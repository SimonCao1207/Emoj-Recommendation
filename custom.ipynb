{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers==4.12.5","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-19T06:46:29.305437Z","iopub.execute_input":"2021-12-19T06:46:29.305909Z","iopub.status.idle":"2021-12-19T06:46:38.105891Z","shell.execute_reply.started":"2021-12-19T06:46:29.305723Z","shell.execute_reply":"2021-12-19T06:46:38.105043Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers==4.12.5 in /opt/conda/lib/python3.7/site-packages (4.12.5)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (2021.11.10)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (0.1.2)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (0.0.46)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (4.8.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (6.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (4.62.3)\nRequirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (0.10.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (1.19.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (21.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (3.3.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.12.5) (2.25.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.5) (3.10.0.2)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.12.5) (3.0.6)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.12.5) (3.6.0)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (2021.10.8)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.12.5) (1.26.7)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (1.16.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (1.1.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.12.5) (8.0.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import necessary libraries","metadata":{}},{"cell_type":"code","source":"\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup, AutoModel, AutoTokenizer \n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import accuracy_score,matthews_corrcoef\n\n#to_preprocessing\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm, trange,tnrange,tqdm_notebook\nimport random\nimport os\nimport io\n%matplotlib inline\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:46:38.108219Z","iopub.execute_input":"2021-12-19T06:46:38.108526Z","iopub.status.idle":"2021-12-19T06:46:46.088048Z","shell.execute_reply.started":"2021-12-19T06:46:38.108485Z","shell.execute_reply":"2021-12-19T06:46:46.087178Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# identify and specify the GPU as the device, later in training loop we will load data into device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == torch.device(\"cuda\"):\n    torch.cuda.manual_seed_all(SEED)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:46:46.089670Z","iopub.execute_input":"2021-12-19T06:46:46.089920Z","iopub.status.idle":"2021-12-19T06:46:46.140841Z","shell.execute_reply.started":"2021-12-19T06:46:46.089882Z","shell.execute_reply":"2021-12-19T06:46:46.140104Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Read dataset","metadata":{}},{"cell_type":"code","source":"# Hugging face dataset\n# df_train = pd.read_csv(\"/kaggle/input/emotions-dataset-for-nlp/train.txt\", delimiter=';', header=None, names=['Text','emotion'])\n# df_test = pd.read_csv(\"/kaggle/input/emotions-dataset-for-nlp/test.txt\", delimiter=';', header=None, names=['Text','emotion'])\n# df_val = pd.read_csv(\"/kaggle/input/emotions-dataset-for-nlp/val.txt\", delimiter=';', header=None, names=['Text','emotion'])\n# df = pd.concat([df_train, df_val, df_test], axis = 0)\n# Kaggle dataset\ndf = pd.read_csv(\"../input/heyooo/out.csv\")\ndf = df.dropna(axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:46:46.142084Z","iopub.execute_input":"2021-12-19T06:46:46.142363Z","iopub.status.idle":"2021-12-19T06:46:46.276803Z","shell.execute_reply.started":"2021-12-19T06:46:46.142296Z","shell.execute_reply":"2021-12-19T06:46:46.276066Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess","metadata":{}},{"cell_type":"code","source":"# Decontract words im -> i am; didnt -> did not\ndef decontracted(phrase):\n    \"\"\"\n    We first define a function to expand the contracted phrase into normal words\n    \"\"\"\n        \n    phrase = re.sub(r\"wont\", \"will not\", phrase)\n    phrase = re.sub(r\"wouldnt\", \"would not\", phrase)\n    phrase = re.sub(r\"shouldnt\", \"should not\", phrase)\n    phrase = re.sub(r\"couldnt\", \"could not\", phrase)\n    phrase = re.sub(r\"cudnt\", \"could not\", phrase)\n    phrase = re.sub(r\"cant\", \"can not\", phrase)\n    phrase = re.sub(r\"dont\", \"do not\", phrase)\n    phrase = re.sub(r\"doesnt\", \"does not\", phrase)\n    phrase = re.sub(r\"didnt\", \"did not\", phrase)\n    phrase = re.sub(r\"wasnt\", \"was not\", phrase)\n    phrase = re.sub(r\"werent\", \"were not\", phrase)\n    phrase = re.sub(r\"havent\", \"have not\", phrase)\n    phrase = re.sub(r\"hadnt\", \"had not\", phrase)\n    phrase = re.sub(r\"neednt\", \"need not\", phrase)\n    phrase = re.sub(r\"isnt\", \"is not\", phrase)\n    phrase = re.sub(r\"arent\", \"are not\", phrase)\n    phrase = re.sub(r\"hasnt\", \"are not\", phrase)\n    \n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n# Get rid of user handles, tags, link, punctuation\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n#     text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = decontracted(text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:46:46.279454Z","iopub.execute_input":"2021-12-19T06:46:46.279957Z","iopub.status.idle":"2021-12-19T06:46:46.293612Z","shell.execute_reply.started":"2021-12-19T06:46:46.279917Z","shell.execute_reply":"2021-12-19T06:46:46.292609Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Apple cleaning function for the dataframe\ndf[\"Text\"] = df[\"Text\"].apply(lambda x: clean_text(x))","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:46:46.296061Z","iopub.execute_input":"2021-12-19T06:46:46.296266Z","iopub.status.idle":"2021-12-19T06:46:47.812748Z","shell.execute_reply.started":"2021-12-19T06:46:46.296232Z","shell.execute_reply":"2021-12-19T06:46:47.811950Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Transform emotion label into encoded label\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndf['label_enc'] = labelencoder.fit_transform(df['emotion'])\ndf.rename(columns={'label':'label_desc'},inplace=True)\ndf.rename(columns={'label_enc':'label'},inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:46:47.813894Z","iopub.execute_input":"2021-12-19T06:46:47.814131Z","iopub.status.idle":"2021-12-19T06:46:47.835126Z","shell.execute_reply.started":"2021-12-19T06:46:47.814100Z","shell.execute_reply":"2021-12-19T06:46:47.834222Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Split dataset\ntrain_text, val_text, train_labels, val_labels = train_test_split(df['Text'], df['label'], \n                                                                    random_state=2021, \n                                                                    test_size=0.1, \n                                                                    stratify=df['label'])\n","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:46:47.836518Z","iopub.execute_input":"2021-12-19T06:46:47.836921Z","iopub.status.idle":"2021-12-19T06:46:47.868300Z","shell.execute_reply.started":"2021-12-19T06:46:47.836885Z","shell.execute_reply":"2021-12-19T06:46:47.867691Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Generate more data by RandomOverSampler","metadata":{}},{"cell_type":"code","source":"# from imblearn.over_sampling import RandomOverSampler\n# ros = RandomOverSampler(random_state=1)\n# train_text, train_labels = ros.fit_resample(np.array(train_text).reshape(-1, 1), np.array(train_labels).reshape(-1, 1))\n# t = [s[0] for s in train_text.tolist()]\n# train_text = t","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:46:47.869505Z","iopub.execute_input":"2021-12-19T06:46:47.869910Z","iopub.status.idle":"2021-12-19T06:46:47.873263Z","shell.execute_reply.started":"2021-12-19T06:46:47.869873Z","shell.execute_reply":"2021-12-19T06:46:47.872428Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"MAX_LEN = 256\n#change to AutoTokenizer if train Bertweet\n# tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", lower = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:46:47.874472Z","iopub.execute_input":"2021-12-19T06:46:47.874886Z","iopub.status.idle":"2021-12-19T06:46:52.285154Z","shell.execute_reply.started":"2021-12-19T06:46:47.874852Z","shell.execute_reply":"2021-12-19T06:46:52.284482Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b80c1478b29441a398edccf1aecabeb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"376752c169e14dd597f834767cd6030d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94358d0d763e4066940337e4fbb16955"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89c5d0b11be148e09893395e926cee68"}},"metadata":{}}]},{"cell_type":"code","source":"# tokenize and encode sequences in the training set (for Bertbase fine tuning)\ntokens_train = tokenizer.batch_encode_plus(\n    train_text.tolist(),\n    max_length = MAX_LEN,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n\n# tokenize and encode sequences in the validation set\ntokens_val = tokenizer.batch_encode_plus(\n    val_text.tolist(),\n    max_length = MAX_LEN,\n    pad_to_max_length=True,\n    truncation=True,\n    return_token_type_ids=False\n)\n# For Bertweet fine tuning\n# # tokenize and encode sequences in the training set\n# tokens_train = tokenizer.batch_encode_plus(\n#     train_text,\n#     padding='longest',\n#     truncation=True,\n#     return_token_type_ids=False\n# )\n\n# # tokenize and encode sequences in the validation set\n# tokens_val = tokenizer.batch_encode_plus(\n#     val_text.tolist(),\n#     padding='longest',\n#     truncation=True,\n#     return_token_type_ids=False\n# )\n","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:46:52.286518Z","iopub.execute_input":"2021-12-19T06:46:52.286777Z","iopub.status.idle":"2021-12-19T06:47:09.540070Z","shell.execute_reply.started":"2021-12-19T06:46:52.286741Z","shell.execute_reply":"2021-12-19T06:47:09.539185Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Create iterator of data with DataLoader","metadata":{}},{"cell_type":"code","source":"# for train set\ntrain_inputs = torch.tensor(tokens_train['input_ids'])\ntrain_y = torch.tensor(train_labels.tolist())\ntrain_masks = torch.tensor(tokens_train['attention_mask'])\n\n\n# for validation set\nval_inputs = torch.tensor(tokens_val['input_ids'])\nval_y = torch.tensor(val_labels.tolist())\nval_masks = torch.tensor(tokens_val['attention_mask'])\n\nbatch_size = 32\ntrain_data = TensorDataset(train_inputs,train_masks,train_y)\ntrain_sampler =  RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nval_data = TensorDataset(val_inputs,val_masks,val_y)\nval_sampler =  RandomSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:47:09.541477Z","iopub.execute_input":"2021-12-19T06:47:09.541718Z","iopub.status.idle":"2021-12-19T06:47:11.341585Z","shell.execute_reply.started":"2021-12-19T06:47:09.541687Z","shell.execute_reply":"2021-12-19T06:47:11.340817Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Load model","metadata":{}},{"cell_type":"code","source":"from transformers import BertModel\n# Use AutoModel for Bertweet\n# bert = AutoModel.from_pretrained(\"vinai/bertweet-base\").to(device)\nbert = BertModel.from_pretrained(\"bert-base-uncased\").to(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:47:11.342878Z","iopub.execute_input":"2021-12-19T06:47:11.343122Z","iopub.status.idle":"2021-12-19T06:47:28.457799Z","shell.execute_reply.started":"2021-12-19T06:47:11.343090Z","shell.execute_reply":"2021-12-19T06:47:28.457054Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f22aba05e0224b7b839468b809c90c2a"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Add fully connected layer on top of the pretrained BERT","metadata":{}},{"cell_type":"code","source":"class BERT_14(torch.nn.Module):\n\n    def __init__(self, bert):\n      super(BERT_14, self).__init__()\n\n      self.bert = bert \n      \n      # dropout layer\n      self.dropout = torch.nn.Dropout(0.1)\n      \n      # relu activation function\n      self.relu =  torch.nn.ReLU()\n\n      # dense layer 1\n      self.fc1 = torch.nn.Linear(768,512)\n      \n      # dense layer 2 (Output layer)\n      self.fc2 = torch.nn.Linear(512,6)\n\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n\n      #pass the inputs to the model  \n      output = self.bert(sent_id,attention_mask=mask)\n      x = output[0][:, 0, :]\n      x = self.fc1(x)\n\n      x = self.relu(x)\n\n      x = self.dropout(x)\n\n      # output layer\n      x = self.fc2(x)\n      \n#       # apply softmax activation\n#       x = F.softmax(x, dim = 1)\n\n      return x","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:47:28.461161Z","iopub.execute_input":"2021-12-19T06:47:28.461410Z","iopub.status.idle":"2021-12-19T06:47:28.468737Z","shell.execute_reply.started":"2021-12-19T06:47:28.461380Z","shell.execute_reply":"2021-12-19T06:47:28.467756Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model = BERT_14(bert)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:47:28.470020Z","iopub.execute_input":"2021-12-19T06:47:28.470508Z","iopub.status.idle":"2021-12-19T06:47:28.495188Z","shell.execute_reply.started":"2021-12-19T06:47:28.470474Z","shell.execute_reply":"2021-12-19T06:47:28.494570Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"BERT_14(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (relu): ReLU()\n  (fc1): Linear(in_features=768, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=6, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Instantiate optimizer and scheduler ","metadata":{}},{"cell_type":"code","source":"epochs = 4\noptimizer = AdamW(model.parameters(),\n                      lr=5e-5,    # Default learning rate\n                      eps=1e-8    # Default epsilon value\n                      )\nnum_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=num_steps)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:47:28.496330Z","iopub.execute_input":"2021-12-19T06:47:28.496759Z","iopub.status.idle":"2021-12-19T06:47:28.505466Z","shell.execute_reply.started":"2021-12-19T06:47:28.496723Z","shell.execute_reply":"2021-12-19T06:47:28.504760Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"\ncriteria = torch.nn.CrossEntropyLoss()\n\ndef train(model, train_dataloader, val_dataloader, epochs):\n    \"\"\"Train the BertClassifier model.\n    \"\"\"\n    # Start training loop\n    print(\"Start training...\\n\")\n    for epoch in range(epochs):\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        model.train()\n        best = 0\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n            optimizer.zero_grad()\n\n            logits = model(b_input_ids, b_attn_mask)\n\n            loss = criteria(logits, b_labels)\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            optimizer.step()\n            scheduler.step()\n            if step % 50 == 0:\n                # Calculate time elapsed for 20 batches\n\n                # Print training results\n                print(\"Train:Epoch \", epoch, \": \", batch_loss/batch_counts) \n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss / len(train_dataloader)\n        val_loss, val_accuracy = evaluate(model, val_dataloader)\n        if val_accuracy > best:\n            best = val_accuracy\n            model_save_name = 'fineTuneModel.bin'\n            path = path_model = F'/kaggle/working/{model_save_name}'\n            torch.save(model.state_dict(),path);\n        print(\"Val loss: \",val_loss,\"; Val accuracy: \", val_accuracy)\n\ndef evaluate(model, val_dataloader):\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = criteria(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:47:28.507872Z","iopub.execute_input":"2021-12-19T06:47:28.508127Z","iopub.status.idle":"2021-12-19T06:47:28.523168Z","shell.execute_reply.started":"2021-12-19T06:47:28.508077Z","shell.execute_reply":"2021-12-19T06:47:28.522333Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train(model, train_dataloader, val_dataloader, epochs=4)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T06:47:28.524601Z","iopub.execute_input":"2021-12-19T06:47:28.525065Z","iopub.status.idle":"2021-12-19T07:39:24.466501Z","shell.execute_reply.started":"2021-12-19T06:47:28.525029Z","shell.execute_reply":"2021-12-19T07:39:24.465705Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Start training...\n\nTrain:Epoch  0 :  1.8096983432769775\nTrain:Epoch  0 :  1.5686814618110656\nTrain:Epoch  0 :  1.318698399066925\nTrain:Epoch  0 :  1.1949644374847412\nTrain:Epoch  0 :  1.1356697273254395\nTrain:Epoch  0 :  1.0901631700992584\nTrain:Epoch  0 :  1.0976531767845155\nTrain:Epoch  0 :  1.0948627483844757\nTrain:Epoch  0 :  1.0352133691310883\nTrain:Epoch  0 :  1.0410805356502533\nTrain:Epoch  0 :  1.0359325444698333\nTrain:Epoch  0 :  1.0541309416294098\nTrain:Epoch  0 :  1.0413427770137786\nTrain:Epoch  0 :  1.0158919978141785\nTrain:Epoch  0 :  1.022891228199005\nTrain:Epoch  0 :  1.0221161258220672\nTrain:Epoch  0 :  1.0059745848178863\nTrain:Epoch  0 :  1.0474246454238891\nTrain:Epoch  0 :  1.024891527891159\nVal loss:  0.98175448651361 ; Val accuracy:  63.65511551155116\nTrain:Epoch  1 :  0.8272722959518433\nTrain:Epoch  1 :  0.8016420602798462\nTrain:Epoch  1 :  0.7723849380016327\nTrain:Epoch  1 :  0.747272869348526\nTrain:Epoch  1 :  0.7729968589544296\nTrain:Epoch  1 :  0.7975544357299804\nTrain:Epoch  1 :  0.8012776947021485\nTrain:Epoch  1 :  0.7503489196300507\nTrain:Epoch  1 :  0.7429176127910614\nTrain:Epoch  1 :  0.7100389933586121\nTrain:Epoch  1 :  0.7963582897186279\nTrain:Epoch  1 :  0.7752784657478332\nTrain:Epoch  1 :  0.7600752335786819\nTrain:Epoch  1 :  0.7286949032545089\nTrain:Epoch  1 :  0.7523311913013458\nTrain:Epoch  1 :  0.7500334477424622\nTrain:Epoch  1 :  0.7405089062452316\nTrain:Epoch  1 :  0.764255588054657\nTrain:Epoch  1 :  0.7365799748897552\nVal loss:  0.9838438125530092 ; Val accuracy:  65.00618811881188\nTrain:Epoch  2 :  0.5080564022064209\nTrain:Epoch  2 :  0.5171141687035561\nTrain:Epoch  2 :  0.5085715273022652\nTrain:Epoch  2 :  0.4665223729610443\nTrain:Epoch  2 :  0.4931282988190651\nTrain:Epoch  2 :  0.46059599667787554\nTrain:Epoch  2 :  0.5287156069278717\nTrain:Epoch  2 :  0.4863229635357857\nTrain:Epoch  2 :  0.49840911269187926\nTrain:Epoch  2 :  0.4739656776189804\nTrain:Epoch  2 :  0.45884654462337493\nTrain:Epoch  2 :  0.4385878506302834\nTrain:Epoch  2 :  0.5032275444269181\nTrain:Epoch  2 :  0.5243554699420929\nTrain:Epoch  2 :  0.4488240414857864\nTrain:Epoch  2 :  0.46120047524571417\nTrain:Epoch  2 :  0.495287321805954\nTrain:Epoch  2 :  0.472313147187233\nTrain:Epoch  2 :  0.47411006927490235\nVal loss:  1.1280035116884968 ; Val accuracy:  64.60396039603961\nTrain:Epoch  3 :  0.43504443764686584\nTrain:Epoch  3 :  0.31589924678206444\nTrain:Epoch  3 :  0.2846574422717094\nTrain:Epoch  3 :  0.3242345441877842\nTrain:Epoch  3 :  0.30234400764107705\nTrain:Epoch  3 :  0.28266871377825736\nTrain:Epoch  3 :  0.2785602195560932\nTrain:Epoch  3 :  0.3286761286854744\nTrain:Epoch  3 :  0.3291240330785513\nTrain:Epoch  3 :  0.34144296161830423\nTrain:Epoch  3 :  0.30291068002581595\nTrain:Epoch  3 :  0.30359846383333206\nTrain:Epoch  3 :  0.2900842525064945\nTrain:Epoch  3 :  0.28863775700330735\nTrain:Epoch  3 :  0.2922174547612667\nTrain:Epoch  3 :  0.29857838690280913\nTrain:Epoch  3 :  0.3100777493417263\nTrain:Epoch  3 :  0.2952817326784134\nTrain:Epoch  3 :  0.2606667125225067\nVal loss:  1.2928706389842648 ; Val accuracy:  63.118811881188115\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Save model","metadata":{}},{"cell_type":"code","source":"\nmodel_save_name = 'fineTuneModel.bin'\npath = F'/kaggle/working/{model_save_name}'\ntorch.save(model.state_dict(),path);","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:39:24.471025Z","iopub.execute_input":"2021-12-19T07:39:24.472922Z","iopub.status.idle":"2021-12-19T07:39:25.507151Z","shell.execute_reply.started":"2021-12-19T07:39:24.472889Z","shell.execute_reply":"2021-12-19T07:39:25.506273Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Load model","metadata":{}},{"cell_type":"code","source":"model = BERT_14(bert)\nmodel.load_state_dict(torch.load(\"./fineTuneModel.bin\"))\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:39:25.508575Z","iopub.execute_input":"2021-12-19T07:39:25.509018Z","iopub.status.idle":"2021-12-19T07:39:25.786338Z","shell.execute_reply.started":"2021-12-19T07:39:25.508978Z","shell.execute_reply":"2021-12-19T07:39:25.785646Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"BERT_14(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (relu): ReLU()\n  (fc1): Linear(in_features=768, out_features=512, bias=True)\n  (fc2): Linear(in_features=512, out_features=6, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Quantitative evaluation","metadata":{}},{"cell_type":"code","source":"save = []\nfor batch in val_dataloader:\n    b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n    with torch.no_grad():\n        logits = model(b_input_ids, b_attn_mask)\n    predict = logits.to(\"cpu\").numpy()\n    predict = np.argmax(predict, axis=1).flatten()\n    labels_flat = b_labels.to(\"cpu\").numpy().flatten()\n    temp = pd.DataFrame({'Actual_class':labels_flat,'Predicted_class':predict})\n    save.append(temp)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:39:25.787641Z","iopub.execute_input":"2021-12-19T07:39:25.788051Z","iopub.status.idle":"2021-12-19T07:39:52.599690Z","shell.execute_reply.started":"2021-12-19T07:39:25.788014Z","shell.execute_reply":"2021-12-19T07:39:52.598978Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# save = []\n# for batch in val_dataloader:\n#     b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n#     with torch.no_grad():\n#         logits = model(b_input_ids, b_attn_mask)\n#     predict = logits.to(\"cpu\").numpy()\n#     predict = np.argmax(predict, axis=1).flatten()\n#     labels_flat = b_labels.to(\"cpu\").numpy().flatten()\n#     temp = pd.DataFrame({'Actual_class':labels_flat,'Predicted_class':predict})\n#     save.append(temp)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:39:52.600827Z","iopub.execute_input":"2021-12-19T07:39:52.601060Z","iopub.status.idle":"2021-12-19T07:39:52.607097Z","shell.execute_reply.started":"2021-12-19T07:39:52.601029Z","shell.execute_reply":"2021-12-19T07:39:52.604838Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"## emotion labels\nlabel2int = {\n  \"sadness\": 4,\n  \"joy\": 2,\n  \"anger\": 0,\n  \"fear\": 1,\n  \"surprise\": 5,\n  \"love\": 3\n}","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:39:52.608183Z","iopub.execute_input":"2021-12-19T07:39:52.608443Z","iopub.status.idle":"2021-12-19T07:39:52.616422Z","shell.execute_reply.started":"2021-12-19T07:39:52.608411Z","shell.execute_reply":"2021-12-19T07:39:52.615696Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"df_metrics = pd.concat(save, axis = 0)\ndf_metrics = df_metrics.reset_index()","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:39:52.617429Z","iopub.execute_input":"2021-12-19T07:39:52.617687Z","iopub.status.idle":"2021-12-19T07:39:52.633563Z","shell.execute_reply.started":"2021-12-19T07:39:52.617655Z","shell.execute_reply":"2021-12-19T07:39:52.632823Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(classification_report(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values, target_names=label2int.keys(), digits=len(label2int)))","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:39:52.634712Z","iopub.execute_input":"2021-12-19T07:39:52.634959Z","iopub.status.idle":"2021-12-19T07:39:52.651621Z","shell.execute_reply.started":"2021-12-19T07:39:52.634926Z","shell.execute_reply":"2021-12-19T07:39:52.650972Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n     sadness   0.754717  0.725624  0.739884       441\n         joy   0.829630  0.828096  0.828862       541\n       anger   0.548732  0.587983  0.567680       699\n        fear   0.526786  0.460938  0.491667       384\n    surprise   0.591078  0.616279  0.603416       516\n        love   0.560582  0.555200  0.557878       625\n\n    accuracy                       0.630381      3206\n   macro avg   0.635254  0.629020  0.631564      3206\nweighted avg   0.630963  0.630381  0.630177      3206\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score,matthews_corrcoef\nprint(accuracy_score(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values))\nprint(matthews_corrcoef(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values))","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:39:52.652616Z","iopub.execute_input":"2021-12-19T07:39:52.652913Z","iopub.status.idle":"2021-12-19T07:39:52.667744Z","shell.execute_reply.started":"2021-12-19T07:39:52.652878Z","shell.execute_reply":"2021-12-19T07:39:52.666983Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"0.6303805364940737\n0.5522949338113854\n","output_type":"stream"}]},{"cell_type":"code","source":"# from sklearn.metrics import classification_report\n# with torch.no_grad():\n#   preds = model(val_inputs.to(device),val_masks.to(device))\n#   preds = preds.detach().cpu().numpy()\n# preds = np.argmax(preds, axis = 1)\n# print(classification_report(val_y, preds))","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:39:52.668851Z","iopub.execute_input":"2021-12-19T07:39:52.669086Z","iopub.status.idle":"2021-12-19T07:39:52.672581Z","shell.execute_reply.started":"2021-12-19T07:39:52.669054Z","shell.execute_reply":"2021-12-19T07:39:52.671630Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncf = confusion_matrix(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:39:52.674036Z","iopub.execute_input":"2021-12-19T07:39:52.674303Z","iopub.status.idle":"2021-12-19T07:39:52.687222Z","shell.execute_reply.started":"2021-12-19T07:39:52.674268Z","shell.execute_reply":"2021-12-19T07:39:52.686461Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(cf, annot=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:39:52.688729Z","iopub.execute_input":"2021-12-19T07:39:52.689270Z","iopub.status.idle":"2021-12-19T07:39:53.066849Z","shell.execute_reply.started":"2021-12-19T07:39:52.689231Z","shell.execute_reply":"2021-12-19T07:39:53.066176Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"import numpy as np\nfrom datasets import load_metric\n\nmetric = load_metric(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:39:53.068182Z","iopub.execute_input":"2021-12-19T07:39:53.068591Z","iopub.status.idle":"2021-12-19T07:39:54.075649Z","shell.execute_reply.started":"2021-12-19T07:39:53.068554Z","shell.execute_reply":"2021-12-19T07:39:54.075004Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.42k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d250dd8fa5a462aafbf032d68a82f9f"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Qualitative evaluation","metadata":{}},{"cell_type":"code","source":"def preprocessing(text):\n    input_id = tokenizer.encode(text, add_special_tokens=True,padding='longest',truncation=True,return_token_type_ids=False)\n    attention = [float(i>0) for i in input_id]\n    input_id = torch.tensor([input_id]).to(device)\n    attention = torch.tensor([attention]).to(device)\n    return input_id, attention","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:39:54.077074Z","iopub.execute_input":"2021-12-19T07:39:54.077391Z","iopub.status.idle":"2021-12-19T07:39:54.082281Z","shell.execute_reply.started":"2021-12-19T07:39:54.077305Z","shell.execute_reply":"2021-12-19T07:39:54.081646Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"text = clean_text(\"I can tell her about the subject\")\ninput, attention = preprocessing(text)\nresult = model(input, attention)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2021-12-19T07:39:54.084164Z","iopub.execute_input":"2021-12-19T07:39:54.084615Z","iopub.status.idle":"2021-12-19T07:39:54.135258Z","shell.execute_reply.started":"2021-12-19T07:39:54.084580Z","shell.execute_reply":"2021-12-19T07:39:54.134481Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"tensor([[ 0.1966,  0.9516, -0.0852, -0.8451, -0.7928,  0.5988]],\n       device='cuda:0', grad_fn=<AddmmBackward>)\n","output_type":"stream"}]}]}